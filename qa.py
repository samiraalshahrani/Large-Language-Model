# -*- coding: utf-8 -*-
"""QA

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OjaGpKlzfgQ6iXcsWII-giXRffC3qwae
"""

!pip install transformers -q # install the huggingface transformers package.

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig

model_name = 'google/flan-t5-base' # the model name in the hugging face repo.
tokenizer = AutoTokenizer.from_pretrained(model_name) # create the tokenizer
model = AutoModelForSeq2SeqLM.from_pretrained(model_name) # craete the model.

line = "what is the capital of kingdom of saudi arabia?"
tokens = tokenizer(line, return_tensors="pt") # convert the text to ids and give us the attention masks.

print(tokens)
config = GenerationConfig(max_new_tokens=200) # let us get output until 200 token (word).
output = model.generate(**tokens, generation_config=config) # generate the output token ids.
print(output)

tokenizer.batch_decode(output, skip_special_tokens=True) # decode the ids to words.